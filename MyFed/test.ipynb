{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np  \n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "from nets.cnn import CNNCifar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self, model_rate):\n",
    "        super(CNNCifar, self).__init__()\n",
    "\n",
    "        pre_hidden_size = [64, 128, 256, 512]\n",
    "        hidden_size=[int(np.ceil(i*model_rate))  for i in pre_hidden_size]\n",
    "        self.hidden_size=hidden_size\n",
    "\n",
    "        self.block1=self._make_block(0)\n",
    "        self.block2=self._make_block(1)\n",
    "        self.block3=self._make_block(2)\n",
    "        self.block4=self._make_block(3)\n",
    "        self.output=nn.Sequential(\n",
    "            nn.Linear(hidden_size[-1], 10)\n",
    "        )\n",
    "        self.flatten=nn.Flatten(1)\n",
    "        #self.logit_projector=nn.utils.parametrizations.orthogonal(nn.Linear(10, 10))\n",
    "        projector_name='orthogonal_projector'\n",
    "        setattr(self, projector_name, nn.utils.parametrizations.orthogonal(\n",
    "            nn.Linear(pre_hidden_size[3], int(np.ceil(0.7*pre_hidden_size[3])))))\n",
    "        projector_name='linear_projector'\n",
    "        setattr(self, projector_name, nn.Linear(pre_hidden_size[3], int(np.ceil(0.7*pre_hidden_size[3]))))\n",
    "\n",
    "    def _make_block(self, layer_idx):\n",
    "        layers=list()\n",
    "        if(layer_idx == 0):\n",
    "            layers.append(nn.Conv2d(3, self.hidden_size[0], 3, 1, 1))\n",
    "        else:\n",
    "            layers.append(nn.Conv2d(self.hidden_size[layer_idx-1], self.hidden_size[layer_idx], 3, 1, 1))\n",
    "        layers.append(nn.BatchNorm2d(self.hidden_size[layer_idx], momentum=None, track_running_stats=False))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        if(layer_idx != 3):\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward_feature(self, x):\n",
    "        out=self.block1(x)\n",
    "        out=self.block2(out)\n",
    "        out=self.block3(out)\n",
    "        out=self.block4(out)\n",
    "        out=nn.AdaptiveAvgPool2d((1, 1))(out)\n",
    "        out=self.flatten(out)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "    \n",
    "    def forward_head(self, x):\n",
    "        out=self.output(x)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.forward_feature(x)\n",
    "        #print(out.shape)\n",
    "        out=self.forward_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train=transforms.Compose([  \n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])  \n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "def cifar10_global(batch_size,root):\n",
    "    dataset_train=datasets.CIFAR10(root, train=True, transform= transform_train, download=True)\n",
    "    dataset_test=datasets.CIFAR10(root, train=False, transform= transform_test, download=True)\n",
    "    dataloader_train=data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_test=data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "teacher_model=CNNCifar(1.0)\n",
    "student_model=CNNCifar(1.0)\n",
    "\n",
    "batch_size=128\n",
    "dataloader_train_global, dataloader_test_global=cifar10_global(batch_size, root='../../data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication Round 1/10\n",
      "Epoch 1/10, Loss: 0.43763516915728673, acc: 0.8128\n",
      "Epoch 2/10, Loss: 0.40561784967742004, acc: 0.8283\n",
      "Epoch 3/10, Loss: 0.3919153481798099, acc: 0.8311\n",
      "Epoch 4/10, Loss: 0.3706304353978628, acc: 0.8302\n",
      "Epoch 5/10, Loss: 0.3565370174853698, acc: 0.8332\n",
      "Epoch 6/10, Loss: 0.33763699915707873, acc: 0.8333\n",
      "Epoch 7/10, Loss: 0.3179672200356603, acc: 0.8441\n",
      "Epoch 8/10, Loss: 0.30787154155619006, acc: 0.8418\n",
      "Epoch 9/10, Loss: 0.29471771278039877, acc: 0.849\n",
      "Epoch 10/10, Loss: 0.2832119171805394, acc: 0.8381\n",
      "Teacher training time: 265.8195023536682\n",
      "Epoch 1/10, Loss: 159.71136358719838, acc: 0.5185\n",
      "Epoch 2/10, Loss: 158.41102233114123, acc: 0.6337\n",
      "Epoch 3/10, Loss: 157.74387330646758, acc: 0.6912\n",
      "Epoch 4/10, Loss: 157.2864949673037, acc: 0.7268\n",
      "Epoch 5/10, Loss: 156.95645566529865, acc: 0.7632\n",
      "Epoch 6/10, Loss: 156.74044027207773, acc: 0.7736\n",
      "Epoch 7/10, Loss: 156.65734245203717, acc: 0.7809\n",
      "Epoch 8/10, Loss: 156.6123147312599, acc: 0.7873\n",
      "Epoch 9/10, Loss: 156.52109286151355, acc: 0.7886\n",
      "Epoch 10/10, Loss: 156.44882318037975, acc: 0.8003\n",
      "Distillation time: 288.75065565109253\n",
      "Communication Round 2/10\n",
      "Epoch 1/10, Loss: 0.27067478567056946, acc: 0.8426\n",
      "Epoch 2/10, Loss: 0.26361432961185877, acc: 0.8454\n",
      "Epoch 3/10, Loss: 0.24715767000489833, acc: 0.8496\n",
      "Epoch 4/10, Loss: 0.2416679884504784, acc: 0.8493\n",
      "Epoch 5/10, Loss: 0.23310018875790983, acc: 0.8529\n",
      "Epoch 6/10, Loss: 0.22097470624672483, acc: 0.8484\n",
      "Epoch 7/10, Loss: 0.214509945677217, acc: 0.8514\n",
      "Epoch 8/10, Loss: 0.20759442082756316, acc: 0.8489\n",
      "Epoch 9/10, Loss: 0.19556742397796772, acc: 0.8572\n",
      "Epoch 10/10, Loss: 0.1900711585112545, acc: 0.86\n",
      "Teacher training time: 113.28956151008606\n",
      "Epoch 1/10, Loss: 156.71045694471914, acc: 0.8128\n",
      "Epoch 2/10, Loss: 156.5244559758826, acc: 0.8258\n",
      "Epoch 3/10, Loss: 156.47697641879697, acc: 0.8285\n",
      "Epoch 4/10, Loss: 156.41334668895865, acc: 0.8146\n",
      "Epoch 5/10, Loss: 156.36714674551277, acc: 0.824\n",
      "Epoch 6/10, Loss: 156.32844427567494, acc: 0.8274\n",
      "Epoch 7/10, Loss: 156.29602089411097, acc: 0.831\n",
      "Epoch 8/10, Loss: 156.25967426541484, acc: 0.8342\n",
      "Epoch 9/10, Loss: 156.20135073118573, acc: 0.8382\n",
      "Epoch 10/10, Loss: 156.17275595363182, acc: 0.8367\n",
      "Distillation time: 135.7960262298584\n",
      "Communication Round 3/10\n",
      "Epoch 1/10, Loss: 0.18639180595841248, acc: 0.8594\n",
      "Epoch 2/10, Loss: 0.1776404127935924, acc: 0.8566\n",
      "Epoch 3/10, Loss: 0.17396483688479494, acc: 0.8495\n",
      "Epoch 4/10, Loss: 0.16161768123164505, acc: 0.8507\n",
      "Epoch 5/10, Loss: 0.15724990608365944, acc: 0.8581\n",
      "Epoch 6/10, Loss: 0.15650087417772665, acc: 0.8549\n",
      "Epoch 7/10, Loss: 0.14984468814662044, acc: 0.8636\n",
      "Epoch 8/10, Loss: 0.14470460465954393, acc: 0.8631\n",
      "Epoch 9/10, Loss: 0.1448622423860118, acc: 0.8574\n",
      "Epoch 10/10, Loss: 0.13389873307417421, acc: 0.8549\n",
      "Teacher training time: 169.24761414527893\n",
      "Epoch 1/10, Loss: 156.4933867635606, acc: 0.8425\n",
      "Epoch 2/10, Loss: 156.3052742873566, acc: 0.8447\n",
      "Epoch 3/10, Loss: 156.23982586438143, acc: 0.8269\n",
      "Epoch 4/10, Loss: 156.21799555911292, acc: 0.8359\n",
      "Epoch 5/10, Loss: 156.20247659803945, acc: 0.8366\n",
      "Epoch 6/10, Loss: 156.18594321721716, acc: 0.8382\n",
      "Epoch 7/10, Loss: 156.1609479686882, acc: 0.8322\n",
      "Epoch 8/10, Loss: 156.13006128238726, acc: 0.8414\n",
      "Epoch 9/10, Loss: 156.1165812045713, acc: 0.8428\n",
      "Epoch 10/10, Loss: 156.11975464639784, acc: 0.8363\n",
      "Distillation time: 250.11125779151917\n",
      "Communication Round 4/10\n",
      "Epoch 1/10, Loss: 0.13543581647222, acc: 0.862\n",
      "Epoch 2/10, Loss: 0.12944602597590602, acc: 0.8536\n",
      "Epoch 3/10, Loss: 0.12638622267967295, acc: 0.8587\n",
      "Epoch 4/10, Loss: 0.11935799830900434, acc: 0.8651\n",
      "Epoch 5/10, Loss: 0.11515544446380548, acc: 0.8535\n",
      "Epoch 6/10, Loss: 0.11440789693837886, acc: 0.86\n",
      "Epoch 7/10, Loss: 0.11730453691176136, acc: 0.8579\n",
      "Epoch 8/10, Loss: 0.11202537640929222, acc: 0.861\n",
      "Epoch 9/10, Loss: 0.10258911488115635, acc: 0.8602\n",
      "Epoch 10/10, Loss: 0.10273924516633039, acc: 0.8647\n",
      "Teacher training time: 118.64718317985535\n",
      "Epoch 1/10, Loss: 156.44901130772845, acc: 0.8457\n",
      "Epoch 2/10, Loss: 156.25137676770174, acc: 0.8412\n",
      "Epoch 3/10, Loss: 156.16707099238528, acc: 0.8402\n",
      "Epoch 4/10, Loss: 156.13765571690814, acc: 0.8406\n",
      "Epoch 5/10, Loss: 156.1421398694002, acc: 0.8541\n",
      "Epoch 6/10, Loss: 156.1327275143394, acc: 0.8541\n",
      "Epoch 7/10, Loss: 156.1201380476167, acc: 0.8538\n",
      "Epoch 8/10, Loss: 156.10961759543116, acc: 0.8562\n",
      "Epoch 9/10, Loss: 156.10484777522993, acc: 0.8515\n",
      "Epoch 10/10, Loss: 156.08943292159069, acc: 0.8548\n",
      "Distillation time: 141.45380687713623\n",
      "Communication Round 5/10\n",
      "Epoch 1/10, Loss: 0.1000848661136368, acc: 0.8579\n",
      "Epoch 2/10, Loss: 0.0999980589370136, acc: 0.8664\n",
      "Epoch 3/10, Loss: 0.09389133430788736, acc: 0.8609\n",
      "Epoch 4/10, Loss: 0.09814514743778711, acc: 0.8579\n",
      "Epoch 5/10, Loss: 0.09111075893597072, acc: 0.8634\n",
      "Epoch 6/10, Loss: 0.0866900504712025, acc: 0.8648\n",
      "Epoch 7/10, Loss: 0.08624614579388705, acc: 0.8643\n",
      "Epoch 8/10, Loss: 0.08958365986852543, acc: 0.8622\n",
      "Epoch 9/10, Loss: 0.08441032750336715, acc: 0.8641\n",
      "Epoch 10/10, Loss: 0.08320658621580704, acc: 0.8643\n",
      "Teacher training time: 117.03997564315796\n",
      "Epoch 1/10, Loss: 156.4028528913667, acc: 0.8526\n",
      "Epoch 2/10, Loss: 156.16201878801178, acc: 0.8536\n",
      "Epoch 3/10, Loss: 156.10606731945956, acc: 0.8546\n",
      "Epoch 4/10, Loss: 156.09523666961283, acc: 0.8535\n",
      "Epoch 5/10, Loss: 156.10324753387064, acc: 0.855\n",
      "Epoch 6/10, Loss: 156.0540273159365, acc: 0.8486\n",
      "Epoch 7/10, Loss: 156.0470888161961, acc: 0.845\n",
      "Epoch 8/10, Loss: 156.064426084108, acc: 0.8462\n",
      "Epoch 9/10, Loss: 156.0576245271707, acc: 0.8501\n",
      "Epoch 10/10, Loss: 156.06557165218305, acc: 0.8521\n",
      "Distillation time: 140.0667119026184\n",
      "Communication Round 6/10\n",
      "Epoch 1/10, Loss: 0.07637288780344645, acc: 0.8605\n",
      "Epoch 2/10, Loss: 0.08278086564510756, acc: 0.861\n",
      "Epoch 3/10, Loss: 0.08065988855612705, acc: 0.8579\n",
      "Epoch 4/10, Loss: 0.07555833912890433, acc: 0.8591\n",
      "Epoch 5/10, Loss: 0.078031027811053, acc: 0.863\n",
      "Epoch 6/10, Loss: 0.07327065089017229, acc: 0.8618\n",
      "Epoch 7/10, Loss: 0.06917014688162891, acc: 0.8621\n",
      "Epoch 8/10, Loss: 0.06954963230873312, acc: 0.8572\n",
      "Epoch 9/10, Loss: 0.06900071688448949, acc: 0.8537\n",
      "Epoch 10/10, Loss: 0.06633888208366873, acc: 0.8645\n",
      "Teacher training time: 112.40494108200073\n",
      "Epoch 1/10, Loss: 156.3823066421702, acc: 0.8565\n",
      "Epoch 2/10, Loss: 156.14684614350523, acc: 0.8548\n",
      "Epoch 3/10, Loss: 156.08579746680923, acc: 0.8527\n",
      "Epoch 4/10, Loss: 156.06594945207428, acc: 0.8552\n",
      "Epoch 5/10, Loss: 156.0477167443384, acc: 0.8549\n",
      "Epoch 6/10, Loss: 156.05113818977452, acc: 0.8566\n",
      "Epoch 7/10, Loss: 156.03527793401403, acc: 0.8541\n",
      "Epoch 8/10, Loss: 156.0322273350969, acc: 0.8496\n",
      "Epoch 9/10, Loss: 156.02769682678996, acc: 0.8526\n",
      "Epoch 10/10, Loss: 156.02580840678155, acc: 0.8585\n",
      "Distillation time: 134.9468333721161\n",
      "Communication Round 7/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Train teacher model\u001b[39;00m\n\u001b[0;32m     79\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 80\u001b[0m \u001b[43mtrain_teacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Distill knowledge to student model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[75], line 19\u001b[0m, in \u001b[0;36mtrain_teacher\u001b[1;34m(model, dataloader, epochs, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     18\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Aneconda\\envs\\pytorch\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:926\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    925\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the training function for the teacher model\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def train_teacher(model, dataloader, epochs, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        test_acc = test(model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "# Define the distillation function\n",
    "def distill(teacher_model, student_model, dataloader, epochs, criterion, optimizer, device, temperature=2.0, alpha=0.5):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    criterion1 = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_features = teacher_model.forward_feature(inputs)\n",
    "                teacher_outputs = teacher_model.forward_head(teacher_features) \n",
    "            T=2.0\n",
    "            student_features = student_model.forward_feature(inputs)\n",
    "            student_outputs = student_model.forward_head(student_features)\n",
    "            student_features=nn.functional.log_softmax(student_features/T, dim=1)\n",
    "            teacher_features=nn.functional.softmax(teacher_features/T, dim=1)\n",
    "            teacher_outputs=nn.functional.softmax(teacher_outputs/T, dim=1)\n",
    "            student_outputs=nn.functional.log_softmax(student_outputs/T, dim=1)\n",
    "            loss=(T**2)*criterion1(student_features, teacher_features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        test_acc = test(student_model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "# Define loss criterion and optimizers\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.KLDivLoss(reduction='batchmean')\n",
    "teacher_optimizer = torch.optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and distillation process\n",
    "communication_rounds = 10\n",
    "teacher_epochs = 10\n",
    "distill_epochs = 10\n",
    "\n",
    "for round in range(communication_rounds):\n",
    "    print(f\"Communication Round {round+1}/{communication_rounds}\")\n",
    "    # Train teacher model\n",
    "    start=time.time()\n",
    "    train_teacher(teacher_model, dataloader_train_global, teacher_epochs, criterion1, teacher_optimizer, device)\n",
    "    print(f\"Teacher training time: {time.time()-start}\")\n",
    "    # Distill knowledge to student model\n",
    "    distill(teacher_model, student_model, dataloader_test_global, distill_epochs, criterion2, student_optimizer, device)\n",
    "    print(f\"Distillation time: {time.time()-start}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
