{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np  \n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "from nets.cnn import CNNCifar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18 170 107  98 177 182   5 146  12 152]\n",
      "[113  50  68 155  57  82  40 105 186 130]\n",
      "[ 49 136 166 146 198 191 178 156  34  85]\n",
      "[130 122 113  14  61 103 186 168  60 159]\n",
      "[115 127 166  27  12   1 108 120  28 121]\n",
      "[ 53 120 134 113 160 122  35 101 167 148]\n",
      "[  6 127 198  62 163 153 188 103 190  64]\n",
      "[195  58 163  44   8  92 191 124 174 156]\n",
      "[  6  60 130 120  70 165   1 100 155  28]\n",
      "[ 76  13  38 120 168 114 197 130 156 112]\n",
      "[ 39 140  85  77  28 163  81  25  19 148]\n",
      "[ 32 156 149 197 115   1 131  65  24  52]\n",
      "[  2 118 152  95  20  49 183  40  96 140]\n",
      "[183  70 101  11  60  51 167 124  50   5]\n",
      "[175  14  93 139 118  41 131  78  55  75]\n",
      "[ 52  58 154 114  87  19  53 179  81  26]\n",
      "[184 117 157  75 121  32 108  73  33 178]\n",
      "[ 73  76 117  98 154  41  71 140 122 139]\n",
      "[ 79  69  51  18  20 140 148  94  45  46]\n",
      "[108  60  56 184 153  96 113  13 197 119]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "for i in range(20):\n",
    "    selected_clients = np.random.choice(200, 10, replace=False)\n",
    "    print(selected_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNCifar(nn.Module):\n",
    "    def __init__(self, model_rate):\n",
    "        super(CNNCifar, self).__init__()\n",
    "\n",
    "        pre_hidden_size = [64, 128, 256, 512]\n",
    "        hidden_size=[int(np.ceil(i*model_rate))  for i in pre_hidden_size]\n",
    "        self.hidden_size=hidden_size\n",
    "\n",
    "        self.block1=self._make_block(0)\n",
    "        self.block2=self._make_block(1)\n",
    "        self.block3=self._make_block(2)\n",
    "        self.block4=self._make_block(3)\n",
    "        self.output=nn.Sequential(\n",
    "            nn.Linear(hidden_size[-1], 10)\n",
    "        )\n",
    "        self.flatten=nn.Flatten(1)\n",
    "        #self.logit_projector=nn.utils.parametrizations.orthogonal(nn.Linear(10, 10))\n",
    "        projector_name='orthogonal_projector'\n",
    "        setattr(self, projector_name, nn.utils.parametrizations.orthogonal(\n",
    "            nn.Linear(pre_hidden_size[3], int(np.ceil(0.7*pre_hidden_size[3])))))\n",
    "        projector_name='linear_projector'\n",
    "        setattr(self, projector_name, nn.Linear(pre_hidden_size[3], int(np.ceil(0.7*pre_hidden_size[3]))))\n",
    "\n",
    "    def _make_block(self, layer_idx):\n",
    "        layers=list()\n",
    "        if(layer_idx == 0):\n",
    "            layers.append(nn.Conv2d(3, self.hidden_size[0], 3, 1, 1))\n",
    "        else:\n",
    "            layers.append(nn.Conv2d(self.hidden_size[layer_idx-1], self.hidden_size[layer_idx], 3, 1, 1))\n",
    "        layers.append(nn.BatchNorm2d(self.hidden_size[layer_idx], momentum=None, track_running_stats=False))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        if(layer_idx != 3):\n",
    "            layers.append(nn.MaxPool2d(2))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward_feature(self, x):\n",
    "        out=self.block1(x)\n",
    "        out=self.block2(out)\n",
    "        out=self.block3(out)\n",
    "        out=self.block4(out)\n",
    "        out=nn.AdaptiveAvgPool2d((1, 1))(out)\n",
    "        out=self.flatten(out)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "    \n",
    "    def forward_head(self, x):\n",
    "        out=self.output(x)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out=self.forward_feature(x)\n",
    "        #print(out.shape)\n",
    "        out=self.forward_head(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train=transforms.Compose([  \n",
    "    transforms.RandomCrop(32, padding=4),  \n",
    "    transforms.RandomHorizontalFlip(),  \n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])  \n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "def cifar10_global(batch_size,root):\n",
    "    dataset_train=datasets.CIFAR10(root, train=True, transform= transform_train, download=True)\n",
    "    dataset_test=datasets.CIFAR10(root, train=False, transform= transform_test, download=True)\n",
    "    dataloader_train=data.DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_test=data.DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader_train, dataloader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "teacher_model=CNNCifar(0.7)\n",
    "student_model=CNNCifar(1.0)\n",
    "\n",
    "batch_size=128\n",
    "dataloader_train_global, dataloader_test_global=cifar10_global(batch_size, root='../../data/cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication Round 1/10\n",
      "Epoch 1/10, Loss: 1.2938785170350233, acc: 0.6189\n",
      "Epoch 2/10, Loss: 0.9349009609588271, acc: 0.6871\n",
      "Epoch 3/10, Loss: 0.8005198968950745, acc: 0.712\n",
      "Epoch 4/10, Loss: 0.7175334631024725, acc: 0.7507\n",
      "Epoch 5/10, Loss: 0.6621169444087827, acc: 0.7662\n",
      "Epoch 6/10, Loss: 0.6160329605459862, acc: 0.7723\n",
      "Epoch 7/10, Loss: 0.5832617718088048, acc: 0.7877\n",
      "Epoch 8/10, Loss: 0.5479911274617285, acc: 0.7965\n",
      "Epoch 9/10, Loss: 0.5205248168972142, acc: 0.7898\n",
      "Epoch 10/10, Loss: 0.49528206301772076, acc: 0.798\n",
      "Teacher training time: 127.73913908004761\n",
      "Epoch 1/10, Loss: 141.07748046102404, acc: 0.5244\n",
      "Epoch 2/10, Loss: 139.95803736433197, acc: 0.6249\n",
      "Epoch 3/10, Loss: 139.47818746446055, acc: 0.6865\n",
      "Epoch 4/10, Loss: 139.1932224321969, acc: 0.7205\n",
      "Epoch 5/10, Loss: 139.0360431912579, acc: 0.7367\n",
      "Epoch 6/10, Loss: 138.98013189774525, acc: 0.7385\n",
      "Epoch 7/10, Loss: 138.94964947277987, acc: 0.7414\n",
      "Epoch 8/10, Loss: 138.91169506990457, acc: 0.7489\n",
      "Epoch 9/10, Loss: 138.87577607360066, acc: 0.7638\n",
      "Epoch 10/10, Loss: 138.8386128099659, acc: 0.7626\n",
      "Distillation time: 197.3418323993683\n",
      "Communication Round 2/10\n",
      "Epoch 1/10, Loss: 0.4755638306555541, acc: 0.8104\n",
      "Epoch 2/10, Loss: 0.4576042096328248, acc: 0.8113\n",
      "Epoch 3/10, Loss: 0.43880968580922813, acc: 0.8149\n",
      "Epoch 4/10, Loss: 0.4229083687371915, acc: 0.826\n",
      "Epoch 5/10, Loss: 0.41006295287700567, acc: 0.813\n",
      "Epoch 6/10, Loss: 0.39007875151798854, acc: 0.8213\n",
      "Epoch 7/10, Loss: 0.3839923455129804, acc: 0.8214\n",
      "Epoch 8/10, Loss: 0.37007603605689904, acc: 0.8293\n",
      "Epoch 9/10, Loss: 0.35590101690853343, acc: 0.8284\n",
      "Epoch 10/10, Loss: 0.3413137425775723, acc: 0.8349\n",
      "Teacher training time: 128.84552145004272\n",
      "Epoch 1/10, Loss: 139.19510119474387, acc: 0.7731\n",
      "Epoch 2/10, Loss: 139.0649657430528, acc: 0.7809\n",
      "Epoch 3/10, Loss: 138.99609336370153, acc: 0.7959\n",
      "Epoch 4/10, Loss: 138.9545714945733, acc: 0.794\n",
      "Epoch 5/10, Loss: 138.92493255228936, acc: 0.7994\n",
      "Epoch 6/10, Loss: 138.89687936517257, acc: 0.797\n",
      "Epoch 7/10, Loss: 138.85570670984967, acc: 0.8018\n",
      "Epoch 8/10, Loss: 138.83613451221322, acc: 0.7991\n",
      "Epoch 9/10, Loss: 138.81557242477996, acc: 0.8041\n",
      "Epoch 10/10, Loss: 138.79678344726562, acc: 0.806\n",
      "Distillation time: 197.10632348060608\n",
      "Communication Round 3/10\n",
      "Epoch 1/10, Loss: 0.3336848758370675, acc: 0.8326\n",
      "Epoch 2/10, Loss: 0.32619846507411476, acc: 0.8339\n",
      "Epoch 3/10, Loss: 0.3140714396448696, acc: 0.8305\n",
      "Epoch 4/10, Loss: 0.30681408629240586, acc: 0.8331\n",
      "Epoch 5/10, Loss: 0.3011558744151269, acc: 0.8369\n",
      "Epoch 6/10, Loss: 0.2879939302992638, acc: 0.841\n",
      "Epoch 7/10, Loss: 0.28504745646968216, acc: 0.839\n",
      "Epoch 8/10, Loss: 0.2766826755707831, acc: 0.835\n",
      "Epoch 9/10, Loss: 0.26645528121143963, acc: 0.8307\n",
      "Epoch 10/10, Loss: 0.2576575005412712, acc: 0.8395\n",
      "Teacher training time: 129.48973321914673\n",
      "Epoch 1/10, Loss: 139.0981864446326, acc: 0.8174\n",
      "Epoch 2/10, Loss: 138.9532248581512, acc: 0.818\n",
      "Epoch 3/10, Loss: 138.8948333353936, acc: 0.8191\n",
      "Epoch 4/10, Loss: 138.88014955158476, acc: 0.8155\n",
      "Epoch 5/10, Loss: 138.86507831042326, acc: 0.8179\n",
      "Epoch 6/10, Loss: 138.85265050960493, acc: 0.8214\n",
      "Epoch 7/10, Loss: 138.83546911312055, acc: 0.8153\n",
      "Epoch 8/10, Loss: 138.83114005945905, acc: 0.8151\n",
      "Epoch 9/10, Loss: 138.82732091976118, acc: 0.8155\n",
      "Epoch 10/10, Loss: 138.82263144963903, acc: 0.8117\n",
      "Distillation time: 198.16468477249146\n",
      "Communication Round 4/10\n",
      "Epoch 1/10, Loss: 0.2564440875711953, acc: 0.8397\n",
      "Epoch 2/10, Loss: 0.24876915326203836, acc: 0.844\n",
      "Epoch 3/10, Loss: 0.24217216801993988, acc: 0.8395\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Train teacher model\u001b[39;00m\n\u001b[0;32m     80\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 81\u001b[0m \u001b[43mtrain_teacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_train_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTeacher training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Distill knowledge to student model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m, in \u001b[0;36mtrain_teacher\u001b[1;34m(model, dataloader, epochs, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 26\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m test(model, dataloader_test_global, device)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the training function for the teacher model\n",
    "def test(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def train_teacher(model, dataloader, epochs, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        test_acc = test(model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "# Define the distillation function\n",
    "def distill(teacher_model, student_model, dataloader, epochs, criterion, optimizer, device, temperature=2.0, alpha=0.5):\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    criterion1 = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_features = teacher_model.forward_feature(inputs)\n",
    "                teacher_outputs = teacher_model.forward_head(teacher_features) \n",
    "            T=2.0\n",
    "            student_features = student_model.forward_feature(inputs)\n",
    "            student_outputs = student_model.forward_head(student_features)\n",
    "            student_features = student_model.orthogonal_projector(student_features)\n",
    "            student_features=nn.functional.log_softmax(student_features/T, dim=1)\n",
    "            teacher_features=nn.functional.softmax(teacher_features/T, dim=1)\n",
    "            teacher_outputs=nn.functional.softmax(teacher_outputs/T, dim=1)\n",
    "            student_outputs=nn.functional.log_softmax(student_outputs/T, dim=1)\n",
    "            loss=(T**2)*criterion1(student_features, teacher_features)+ (T**2)*criterion(student_outputs, teacher_outputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        test_acc = test(student_model, dataloader_test_global, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}, acc: {test_acc}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "teacher_model.to(device)\n",
    "student_model.to(device)\n",
    "\n",
    "# Define loss criterion and optimizers\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.KLDivLoss(reduction='batchmean')\n",
    "teacher_optimizer = torch.optim.Adam(teacher_model.parameters(), lr=0.001)\n",
    "student_optimizer = torch.optim.Adam(student_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and distillation process\n",
    "communication_rounds = 10\n",
    "teacher_epochs = 10\n",
    "distill_epochs = 10\n",
    "\n",
    "for round in range(communication_rounds):\n",
    "    print(f\"Communication Round {round+1}/{communication_rounds}\")\n",
    "    # Train teacher model\n",
    "    start=time.time()\n",
    "    train_teacher(teacher_model, dataloader_train_global, teacher_epochs, criterion1, teacher_optimizer, device)\n",
    "    print(f\"Teacher training time: {time.time()-start}\")\n",
    "    # Distill knowledge to student model\n",
    "    distill(teacher_model, student_model, dataloader_test_global, distill_epochs, criterion2, student_optimizer, device)\n",
    "    print(f\"Distillation time: {time.time()-start}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
